{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import KAN, LBFGS\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import autograd\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fun(x_int, x_bc, xdim, model):\n",
    "    mu = 1\n",
    "    h = 1e-5  # Small interval for finite differences\n",
    "\n",
    "    x, t = x_int[:, :-1], x_int[:, -1:]\n",
    "    t.requires_grad_()\n",
    "    x.requires_grad_()\n",
    "\n",
    "    # Combine x and t to create input tensor for the model\n",
    "    input_tensor = torch.cat((x, t), dim=1)\n",
    "    u = model(input_tensor)\n",
    "\n",
    "    # Initialize tensors for derivatives\n",
    "    du_dt = torch.zeros_like(u)\n",
    "    du_dx = torch.zeros_like(x)\n",
    "    d2u_dx2 = torch.zeros_like(x)\n",
    "\n",
    "    # First-order time derivative using Euler's approximation\n",
    "    # for i in range(50):\n",
    "    #     # Compute u(t+h) and u(t-h)\n",
    "    #     t_forward = (t[i] + h).view(1, 1)  # Ensure shape is [1, 1]\n",
    "    #     t_backward = (t[i] - h).view(1, 1)  # Ensure shape is [1, 1]\n",
    "    #     u_forward = model(torch.cat((x[i:i+1], t_forward), dim=1))\n",
    "    #     u_backward = model(torch.cat((x[i:i+1], t_backward), dim=1))\n",
    "\n",
    "    #     # Euler's approximation for first derivative\n",
    "    #     du_dt[i] = (u_forward - u_backward) / (2 * h)\n",
    "    du_dt = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(t), create_graph=True)[0]\n",
    "    \n",
    "\n",
    "    # First-order spatial derivative using Euler's approximation\n",
    "    # for i in range(50):\n",
    "    #     for j in range(3):  # Loop through each dimension of x\n",
    "    #         # Compute u(x+h) and u(x-h)\n",
    "    #         x_forward = x[i:i+1].clone()\n",
    "    #         x_backward = x[i:i+1].clone()\n",
    "    #         x_forward[0, j] += h\n",
    "    #         x_backward[0, j] -= h\n",
    "            \n",
    "    #         u_forward = model(torch.cat((x_forward, t[i:i+1]), dim=1))\n",
    "    #         u_backward = model(torch.cat((x_backward, t[i:i+1]), dim=1))\n",
    "\n",
    "    #         # Euler's approximation for first derivative\n",
    "    #         du_dx[i, j] = (u_forward - u_backward) / (2 * h)\n",
    "    du_dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, allow_unused=True)[0]\n",
    "\n",
    "    # Second-order spatial derivative using Euler's approximation\n",
    "    for i in range(50):\n",
    "        for j in range(3):  # Loop through each dimension of x\n",
    "            # Compute u(x+h), u(x), and u(x-h)\n",
    "            x_forward = x[i:i+1].clone()\n",
    "            x_backward = x[i:i+1].clone()\n",
    "            x_forward[0, j] += h\n",
    "            x_backward[0, j] -= h\n",
    "            \n",
    "            u_forward = model(torch.cat((x_forward, t[i:i+1]), dim=1))\n",
    "            u_current = u[i]\n",
    "            u_backward = model(torch.cat((x_backward, t[i:i+1]), dim=1))\n",
    "\n",
    "            # Euler's approximation for second derivative\n",
    "            d2u_dx2[i, j] = (u_forward - 2 * u_current + u_backward) / (h ** 2)\n",
    "\n",
    "    # Compute the residual R_int\n",
    "    R_int = torch.mean((du_dt.squeeze(1) + torch.sum(d2u_dx2, dim=1) - mu * torch.sum(du_dx ** 2, dim=1)) ** 2)\n",
    "\n",
    "    # Boundary condition handling\n",
    "    x_bc, t_bc = x_bc[:, :-1], x_bc[:, -1:]\n",
    "    t_bc.requires_grad_()\n",
    "    input_tensor_bc = torch.cat((x_bc, t_bc), dim=1)\n",
    "    u_bc = model(input_tensor_bc)\n",
    "\n",
    "    R_bc = torch.mean(torch.square(u_bc - torch.log((1 + torch.norm(x_bc, p=2) ** 2) / 2)))\n",
    "\n",
    "    return R_int, R_bc\n",
    "\n",
    "\n",
    "\n",
    "# def loss_fun(x_int, x_bc, xdim, model):\n",
    "#     mu = 1\n",
    "#     # print(model)\n",
    "#     x, t = x_int[:, :-1], x_int[:, -1:]\n",
    "#     # print(\"Shape of x: \" + str(x.shape) + \", Shape of t: \" + str(t.shape))\n",
    "#     t.requires_grad_()\n",
    "#     x.requires_grad_()\n",
    "\n",
    "#     input_tensor = torch.cat((x, t), dim=1)\n",
    "#     # print(\"Shape of input tensor: \" + str(input_tensor.shape))\n",
    "#     u = model(input_tensor)\n",
    "#     # print(\"Shape of output tensor: \" + str(u.shape))\n",
    "\n",
    "#     du_dt = torch.zeros_like(u)\n",
    "\n",
    "#     # Loop through each of the 50 values\n",
    "#     for i in range(50):\n",
    "#         if i == 0:\n",
    "#             # Forward difference for the first element\n",
    "#             delta_t = t[i + 1] - t[i]\n",
    "#             du_dt[i] = (u[i + 1] - u[i]) / delta_t\n",
    "#         elif i == 49:\n",
    "#             # Backward difference for the last element\n",
    "#             delta_t = t[i] - t[i - 1]\n",
    "#             du_dt[i] = (u[i] - u[i - 1]) / delta_t\n",
    "#         else:\n",
    "#             # Central difference for the interior elements\n",
    "#             delta_t_forward = t[i + 1] - t[i]\n",
    "#             delta_t_backward = t[i] - t[i - 1]\n",
    "#             du_dt[i] = (u[i + 1] - u[i - 1]) / (delta_t_forward + delta_t_backward)\n",
    "#     # print(\"Euler's du_dt: \" + str(du_dt[:10]))\n",
    "\n",
    "#     # du_dt = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(t), create_graph=True)[0]\n",
    "#     # print(\"Autograd's du_dt: \" + str(du_dt[:10]))\n",
    "\n",
    "#     # Display the first-order derivative array\n",
    "#     # print(\"First-order derivative of u with respect to t:\")\n",
    "#     # print(du_dt)\n",
    "\n",
    "#     # First-order time derivative using finite differences\n",
    "#     # du_dt = torch.zeros_like(u)\n",
    "\n",
    "#     # for i in range(50):\n",
    "#     #     if i == 0:\n",
    "#     #         # Forward difference for the first element\n",
    "#     #         delta_t = t[i + 1] - t[i]\n",
    "#     #         du_dt[i] = (u[i + 1] - u[i]) / delta_t\n",
    "#     #     elif i == 49:\n",
    "#     #         # Backward difference for the last element\n",
    "#     #         delta_t = t[i] - t[i - 1]\n",
    "#     #         du_dt[i] = (u[i] - u[i - 1]) / delta_t\n",
    "#     #     else:\n",
    "#     #         # Central difference for the interior elements\n",
    "#     #         delta_t_forward = t[i + 1] - t[i]\n",
    "#     #         delta_t_backward = t[i] - t[i - 1]\n",
    "#     #         du_dt[i] = (u[i + 1] - u[i - 1]) / (delta_t_forward + delta_t_backward)\n",
    "#     # print(\"Shape of du_dt: \" + str(du_dt.shape))\n",
    "#     # print(du_dt)\n",
    "\n",
    "#     # First-order spatial derivative using finite differences\n",
    "#     # du_dx = torch.zeros_like(u)\n",
    "\n",
    "#     # for i in range(50):\n",
    "#     #     if i == 0:\n",
    "#     #         # Forward difference for the first element\n",
    "#     #         delta_x = x[i + 1] - x[i]\n",
    "#     #         du_dx[i] = (u[i + 1] - u[i]) / delta_x\n",
    "#     #     elif i == 49:\n",
    "#     #         # Backward difference for the last element\n",
    "#     #         delta_x = x[i] - x[i - 1]\n",
    "#     #         du_dx[i] = (u[i] - u[i - 1]) / delta_x\n",
    "#     #     else:\n",
    "#     #         # Central difference for the interior elements\n",
    "#     #         delta_x_forward = x[i + 1] - x[i]\n",
    "#     #         delta_x_backward = x[i] - x[i - 1]\n",
    "#     #         du_dx[i] = (u[i + 1] - u[i - 1]) / (delta_x_forward + delta_x_backward)\n",
    "#     # print(\"Shape of du_dx: \" + str(du_dx.shape))\n",
    "\n",
    "#     du_dx = torch.zeros_like(x)\n",
    "\n",
    "#     # Loop through each of the 50 values\n",
    "#     for i in range(50):\n",
    "#         for j in range(3):  # Loop through each dimension of x\n",
    "#             if i == 0:\n",
    "#                 # Forward difference for the first element\n",
    "#                 delta_x = x[i + 1, j] - x[i, j]\n",
    "#                 du_dx[i, j] = (u[i + 1] - u[i]) / delta_x\n",
    "#             elif i == 49:\n",
    "#                 # Backward difference for the last element\n",
    "#                 delta_x = x[i, j] - x[i - 1, j]\n",
    "#                 du_dx[i, j] = (u[i] - u[i - 1]) / delta_x\n",
    "#             else:\n",
    "#                 # Central difference for the interior elements\n",
    "#                 delta_x_forward = x[i + 1, j] - x[i, j]\n",
    "#                 delta_x_backward = x[i, j] - x[i - 1, j]\n",
    "#                 du_dx[i, j] = (u[i + 1] - u[i - 1]) / (delta_x_forward + delta_x_backward)\n",
    "\n",
    "\n",
    "#     d2u_dx2 = torch.zeros_like(x)\n",
    "\n",
    "#     # Loop through each dimension of x (3 dimensions in this case)\n",
    "#     for j in range(3):\n",
    "#         # Loop through each of the 50 values\n",
    "#         for i in range(50):\n",
    "#             if i == 0:\n",
    "#                 # Forward difference for the first element\n",
    "#                 delta_x = x[i + 1, j] - x[i, j]\n",
    "#                 d2u_dx2[i, j] = (u[i + 2] - 2 * u[i + 1] + u[i]) / (delta_x ** 2)\n",
    "#             elif i == 49:\n",
    "#                 # Backward difference for the last element\n",
    "#                 delta_x = x[i, j] - x[i - 1, j]\n",
    "#                 d2u_dx2[i, j] = (u[i] - 2 * u[i - 1] + u[i - 2]) / (delta_x ** 2)\n",
    "#             else:\n",
    "#                 # Central difference for the interior elements\n",
    "#                 delta_x_forward = x[i + 1, j] - x[i, j]\n",
    "#                 delta_x_backward = x[i, j] - x[i - 1, j]\n",
    "#                 delta_x = (delta_x_forward + delta_x_backward) / 2\n",
    "                \n",
    "#                 u_plus_h = u[i + 1]\n",
    "#                 u_current = u[i]\n",
    "#                 u_minus_h = u[i - 1]\n",
    "                \n",
    "#                 d2u_dx2[i, j] = (u_plus_h - 2 * u_current + u_minus_h) / (delta_x ** 2)\n",
    "\n",
    "#     # Display the second-order derivative tensor\n",
    "#     # print(\"Second-order derivative of u with respect to each dimension of x:\")\n",
    "#     # print(d2u_dx2)\n",
    "\n",
    "#     # # Second-order spatial derivative using finite differences\n",
    "#     # d2u_dx2 = torch.zeros_like(u)\n",
    "#     # print(\"Printing all values for inspection:\")\n",
    "#     # print(\"u[0]: \" + str(u[0]))\n",
    "#     # print(\"u[1]: \" + str(u[1]))\n",
    "#     # print(\"u[2]: \" + str(u[2]))\n",
    "#     # print(\"x[0]: \" + str(x[0]))\n",
    "#     # print(\"x[1]: \" + str(x[1]))\n",
    "#     # print(\"delta_x: \" + str(x[1] - x[0]))\n",
    "#     # print()\n",
    "#     # # Loop through each of the 50 values\n",
    "#     # for i in range(50):\n",
    "#     #     if i == 0:\n",
    "#     #         # Forward difference for the first element\n",
    "#     #         print(\"Before tensor operation\")\n",
    "#     #         delta_x = x[i + 1] - x[i]\n",
    "#     #         print(\"After tensor operation\")\n",
    "#     #         d2u_dx2[i] = (u[i + 2] - 2 * u[i + 1] + u[i]) / (delta_x ** 2)\n",
    "#     #         print(\"After tensor operation 1\")\n",
    "#     #     elif i == 49:\n",
    "#     #         # Backward difference for the last element\n",
    "#     #         delta_x = x[i] - x[i - 1]\n",
    "#     #         print(\"After tensor operation 2\")\n",
    "#     #         d2u_dx2[i] = (u[i] - 2 * u[i - 1] + u[i - 2]) / (delta_x ** 2)\n",
    "#     #         print(\"After tensor operation 3\")\n",
    "#     #     else:\n",
    "#     #         # Central difference for the interior elements\n",
    "#     #         delta_x_forward = x[i + 1] - x[i]\n",
    "#     #         print(\"After tensor operation 4\")\n",
    "#     #         delta_x_backward = x[i] - x[i - 1]\n",
    "#     #         print(\"After tensor operation 5\")\n",
    "#     #         delta_x = (delta_x_forward + delta_x_backward) / 2\n",
    "#     #         print(\"After tensor operation 6\")\n",
    "            \n",
    "#     #         u_plus_h = u[i + 1]\n",
    "#     #         u_current = u[i]\n",
    "#     #         u_minus_h = u[i - 1]\n",
    "#     #         print(\"After tensor operation 7\")\n",
    "#     #         d2u_dx2[i] = (u_plus_h - 2 * u_current + u_minus_h) / (delta_x ** 2)\n",
    "#     #         print(\"After tensor operation 8\")\n",
    "\n",
    "#     # # Display the second-order derivative tensor\n",
    "#     # print(\"Second-order derivative of u with respect to x:\")\n",
    "#     # print(d2u_dx2)\n",
    "#     # print(\"Shape of d2u_dx2: \" + str(d2u_dx2.shape))\n",
    "\n",
    "#     # Compute the residual R_int\n",
    "#     R_int = torch.mean((du_dt.squeeze(1) + torch.sum(d2u_dx2, dim=1) - mu*torch.sum(du_dx**2, dim=1))**2)\n",
    "#     # R_int = torch.mean((du_dt.squeeze(1) + d2u_dx2.squeeze(1) - mu * (du_dx.squeeze(1) ** 2)) ** 2)\n",
    "\n",
    "#     # Boundary condition handling\n",
    "#     x_bc, t_bc = x_bc[:, :-1], x_bc[:, -1:]\n",
    "#     t_bc.requires_grad_()\n",
    "#     input_tensor_bc = torch.cat((x_bc, t_bc), dim=1)\n",
    "#     u_bc = model(input_tensor_bc)\n",
    "\n",
    "#     R_bc = torch.mean(torch.square(u_bc - torch.log((1 + torch.norm(x_bc, p=2) ** 2) / 2)))\n",
    "\n",
    "#     return R_int, R_bc\n",
    "\n",
    "\n",
    "\n",
    "# # def loss_fun(x_int, x_bc, xdim, model):\n",
    "\n",
    "# #     mu = 1\n",
    "\n",
    "# #     x, t = x_int[:, :-1], x_int[:, -1:]\n",
    "# #     print(\"Shape of x: \" + str(x.shape) + \", Shape of t: \" + str(t.shape))\n",
    "# #     t.requires_grad_()\n",
    "# #     x.requires_grad_()\n",
    "\n",
    "# #     input_tensor = torch.cat((x, t), dim=1)\n",
    "# #     print(\"Shape of input tensor: \" + str(input_tensor.shape))\n",
    "# #     u = model(input_tensor)\n",
    "# #     print(\"Shape of output tensor: \" + str(u.shape))\n",
    "\n",
    "\n",
    "# #     du_dt = torch.zeros_like(u)\n",
    "\n",
    "# #     # Loop through each of the 50 values\n",
    "# #     for i in range(50):\n",
    "# #         if i == 0:\n",
    "# #             # Forward difference for the first element\n",
    "# #             delta_t = t[i + 1] - t[i]\n",
    "# #             du_dt[i] = (u[i + 1] - u[i]) / delta_t\n",
    "# #         elif i == 49:\n",
    "# #             # Backward difference for the last element\n",
    "# #             delta_t = t[i] - t[i - 1]\n",
    "# #             du_dt[i] = (u[i] - u[i - 1]) / delta_t\n",
    "# #         else:\n",
    "# #             # Central difference for the interior elements\n",
    "# #             delta_t_forward = t[i + 1] - t[i]\n",
    "# #             delta_t_backward = t[i] - t[i - 1]\n",
    "# #             du_dt[i] = (u[i + 1] - u[i - 1]) / (delta_t_forward + delta_t_backward)\n",
    "# #     # print(\"Shape of t: \" + str(t.shape))\n",
    "# #     # du_dt = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(t), create_graph=True)[0]\n",
    "# #     print(\"Shape of du_dt: \" + str(du_dt.shape))\n",
    "\n",
    "\n",
    "# #     du_dx = torch.zeros_like(u)\n",
    "\n",
    "# #     # Loop through each of the 50 values\n",
    "# #     for i in range(50):\n",
    "# #         if i == 0:\n",
    "# #             # Forward difference for the first element\n",
    "# #             delta_x = x[i + 1] - x[i]\n",
    "# #             du_dx[i] = (u[i + 1] - u[i]) / delta_x\n",
    "# #         elif i == 49:\n",
    "# #             # Backward difference for the last element\n",
    "# #             delta_x = x[i] - x[i - 1]\n",
    "# #             du_dx[i] = (u[i] - u[i - 1]) / delta_x\n",
    "# #         else:\n",
    "# #             # Central difference for the interior elements\n",
    "# #             delta_x_forward = x[i + 1] - x[i]\n",
    "# #             delta_x_backward = x[i] - x[i - 1]\n",
    "# #             du_dx[i] = (u[i + 1] - u[i - 1]) / (delta_x_forward + delta_x_backward)\n",
    "\n",
    "\n",
    "# #     # print(\"Shape of x: \" + str(x.shape))\n",
    "# #     # du_dx = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, allow_unused=True)[0]\n",
    "# #     print(\"Shape of du_dx: \" + str(du_dx.shape))\n",
    "\n",
    "# #     # for i in range(50):\n",
    "# #     #     # Compute the gradient of u[i] with respect to x[i]\n",
    "# #     #         du_dx = torch.autograd.grad(u[i], x[i][j], grad_outputs=torch.ones_like(u[i]), create_graph=True)[0]\n",
    "        \n",
    "# #     #         # Check if the gradient is None (if the tensor wasn't used in the computation)\n",
    "# #     #         if du_dx is not None:\n",
    "# #     #             print(\"Shape of du_dx: \" + str(du_dx.shape))\n",
    "# #     #             # Sum the gradients to get a single scalar value\n",
    "# #     #             grad_sum += du_dx[i]\n",
    "# #     #         else:\n",
    "# #     #             print(\"du_dx was found to be None.\")\n",
    "\n",
    "# #     # print(du_dx)\n",
    "# #     # du_dx = torch.autograd.grad(u, x, create_graph=True)[0].sum()\n",
    "# #     # print(\"Shape of du_dx: \" + str(du_dx.shape))\n",
    "\n",
    "# #     d2u_dx2 = torch.zeros_like(u)\n",
    "\n",
    "# #     # Loop through each of the 50 values\n",
    "# #     for i in range(50):\n",
    "# #         if i == 0:\n",
    "# #             # Forward difference for the first element\n",
    "# #             delta_x_forward = x[i + 1] - x[i]\n",
    "# #             u_plus_h = u[i + 1]\n",
    "# #             u_current = u[i]\n",
    "# #             d2u_dx2[i] = (u_plus_h - 2 * u_current + u_current) / (delta_x_forward ** 2)\n",
    "# #         elif i == 49:\n",
    "# #             # Backward difference for the last element\n",
    "# #             delta_x_backward = x[i] - x[i - 1]\n",
    "# #             u_minus_h = u[i - 1]\n",
    "# #             u_current = u[i]\n",
    "# #             d2u_dx2[i] = (u_current - 2 * u_current + u_minus_h) / (delta_x_backward ** 2)\n",
    "# #         else:\n",
    "# #             # Central difference for the interior elements\n",
    "# #             delta_x_forward = x[i + 1] - x[i]\n",
    "# #             delta_x_backward = x[i] - x[i - 1]\n",
    "# #             delta_x = (delta_x_forward + delta_x_backward) / 2\n",
    "            \n",
    "# #             u_plus_h = u[i + 1]\n",
    "# #             u_current = u[i]\n",
    "# #             u_minus_h = u[i - 1]\n",
    "            \n",
    "# #             d2u_dx2[i] = (u_plus_h - 2 * u_current + u_minus_h) / (delta_x ** 2)\n",
    "\n",
    "# #     # print(d2u_dx2.shape)\n",
    "# #     # print(d2u_dx2)\n",
    "\n",
    "# #     # print(\"Jacobian Test start\")\n",
    "# #     # d2u_dx2 = torch.autograd.functional.jacobian(lambda x: du_dx, x)\n",
    "# #     # print(\"Jacobian Test end\")\n",
    "\n",
    "# #     print(\"Shape of d2u_dx2: \", d2u_dx2.shape)\n",
    "\n",
    "# #     # print(\"du_dx gradient enabled.\")\n",
    "# #     # du_dx.requires_grad_()\n",
    "# #     # print(du_dx.requires_grad)\n",
    "# #     # print(\"du_dx grad_fn: \", du_dx.grad_fn)\n",
    "# #     # print(\"x grad_fn: \",x.grad_fn)\n",
    "# #     # d2u_dx2 = []\n",
    "# #     # for i in range(xdim):\n",
    "# #     #     # (batch_size, 1)\n",
    "# #     #     # print(\"Gradient calculation start\")\n",
    "# #     #     # temp = du_dx[:, i].sum()\n",
    "# #     #     # print(\"Input Grad:\", temp.requires_grad)\n",
    "# #     #     d2u_dxidxi = torch.autograd.grad(du_dx[:, i].sum(), x, grad_outputs=torch.ones_like(du_dx[:, i]), retain_graph=True, allow_unused=True)[0]\n",
    "# #     #     # [:, i:i+1]\n",
    "# #     #     print(d2u_dxidxi)\n",
    "# #     #     print(\"First gradient calculated.\")\n",
    "# #     #     d2u_dx2.append(d2u_dxidxi)\n",
    "# #     # # (batch_size, x_dim)\n",
    "# #     # d2u_dx2 = torch.concat(d2u_dx2, dim=1)\n",
    "\n",
    "# #     # d2u_dx2 = []\n",
    "# #     # for i in range(xdim):\n",
    "# #     #     d2u_dxidxi = torch.autograd.grad(du_dx[:, i].sum(), x, retain_graph=True, allow_unused=True)[0][:, i:i+1]\n",
    "# #     #     if d2u_dxidxi is None:\n",
    "# #     #         raise RuntimeError(f'Gradient w.r.t x[:, {i}] is None')\n",
    "# #     #     d2u_dx2.append(d2u_dxidxi)\n",
    "\n",
    "# #     # d2u_dx2 = torch.cat(d2u_dx2, dim=1)\n",
    "\n",
    "# #     R_int = torch.mean((du_dt.squeeze(1) + torch.sum(d2u_dx2, dim=1) - mu*torch.sum(du_dx**2, dim=1))**2)\n",
    "# #     # R_int = torch.mean((du_dt.squeeze(1) - mu*torch.sum(du_dx**2, dim=1))**2)\n",
    "\n",
    "# #     x, t = x_bc[:, :-1], x_bc[:, -1:]\n",
    "# #     t.requires_grad_()\n",
    "# #     input_tensor = torch.cat((x, t), dim=1)\n",
    "# #     u_bc = model(input_tensor)   \n",
    "\n",
    "# #     R_bc =  torch.mean(torch.square(u_bc - torch.log((1 + torch.norm(x, p=2) ** 2) / 2)))\n",
    "\n",
    "# #     return R_int, R_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    global xdim, T\n",
    "    \n",
    "    xdim = 3\n",
    "    T = 1\n",
    "\n",
    "    tensor1 = torch.randn((50, xdim), device=device)\n",
    "    tensor2 = torch.rand((50, 1), device=device)*T\n",
    "    x_int = torch.cat([tensor1, tensor2], dim=1)\n",
    "\n",
    "    # print(\"tensor1.shape: \", tensor1.shape)\n",
    "    # print(\"tensor2.shape: \", tensor2.shape)\n",
    "    # print(\"x_int.shape: \", x_int.shape)\n",
    "    \n",
    "    tensor1 = torch.randn((50, xdim), device=device)\n",
    "    tensor2 = torch.ones((50, 1), device=device)*T\n",
    "    x_bc = torch.cat([tensor1, tensor2], dim=1)\n",
    "    \n",
    "    return x_int, x_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_b       = 10.0\n",
    "lambda_ic      =10.0\n",
    "\n",
    "steps = 40\n",
    "alpha = 0.1\n",
    "log = 1\n",
    "\n",
    "N = 50\n",
    "xdim = 3\n",
    "\n",
    "global loss_int_hist, loss_bc_hist, loss_ic_hist, pred_hist\n",
    "\n",
    "pred_hist      = np.zeros(N)\n",
    "\n",
    "model = KAN(width=[xdim+1,100, 40, 1], grid=5, k=3, grid_eps=1.0, noise_scale_base=0.25)\n",
    "optimizer = LBFGS(model.parameters(), lr=1, history_size=10, line_search_fn=\"strong_wolfe\", tolerance_grad=1e-32, tolerance_change=1e-32, tolerance_ys=1e-32)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(steps):\n",
    "\n",
    "    loss_int_hist  = np.zeros(steps)\n",
    "    loss_bc_hist    = np.zeros(steps)\n",
    "    # loss_ic_hist    = np.zeros(steps)\n",
    "    \n",
    "    pbar = tqdm(range(steps), desc='description')\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-7)\n",
    "\n",
    "    for epoch in pbar:\n",
    "        def closure():\n",
    "            global loss_int, loss_bc, x_int\n",
    "            # print(\"Hello 1\")\n",
    "            # zero the gradient buffers\n",
    "            optimizer.zero_grad()\n",
    "            # print(\"Hello 2\")\n",
    "            x_int, x_bc = get_data()\n",
    "\n",
    "            # x_int = x_int.reshape(-1, 1)\n",
    "            # x_bc = x_bc.reshape(-1, 1)\n",
    "            \n",
    "            # print(x_int.shape)\n",
    "            # print(x_bc.shape)\n",
    "\n",
    "            # compute losses\n",
    "            # print(\"Hello 3\")\n",
    "            loss_int, loss_bc = loss_fun(x_int, x_bc, xdim, model)\n",
    "            loss = loss_int + lambda_b*loss_bc\n",
    "\n",
    "            # print(\"Hello 4\")\n",
    "            # compute gradients of training loss\n",
    "            loss.backward()\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        # print(\"Hello 5\")\n",
    "        x_int, x_bc = get_data()\n",
    "        # print(x_int.shape)\n",
    "        # print(x_bc.shape)\n",
    "\n",
    "        # if epoch % 5 == 0 and epoch < 50:\n",
    "        #     model.update_grid_from_samples(x_int)\n",
    "\n",
    "        # print(\"Hello 6\")\n",
    "        optimizer.step(closure)\n",
    "        loss = loss_int + lambda_b*loss_bc\n",
    "\n",
    "        # print(\"Hello 7\")\n",
    "        if epoch % log == 0:\n",
    "            pbar.set_description(\"interior pde loss: %.2e | bc loss: %.2e \" % (loss_int.cpu().detach().numpy(), loss_bc.cpu().detach().numpy()))\n",
    "\n",
    "        # print(f'   --- epoch {epoch+1}: loss_int = {loss_int.item():.4e}, loss_bc = {loss_bc.item():.4e}, loss_ic = {loss_ic.item():.4e}')\n",
    "        \n",
    "        # save loss\n",
    "        loss_int_hist[epoch] = loss_int\n",
    "        loss_bc_hist[epoch] = loss_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "train(steps)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Training completed in {elapsed_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
